Skip to main content

Help us build the JS tools that power AI apps at companies like Replit, Uber, LinkedIn, GitLab, and more. Join our team!

Integrations

API Reference

More

v0.3

ðŸ¦œðŸ”—

Chat

Search

K

Providers

Anthropic

AWS

Google

Microsoft

OpenAI

Components

Chat models

LLMs

Embedding models

Document loaders

Vector stores

Retrievers

Tools/Toolkits

Key-value stores

Other

ComponentsTools/ToolkitsTavily Search

TavilySearch



Tavily is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed. Tavily offers two key endpoints, one of which being Search, which provides search results tailored for LLMs and RAG.



This guide provides a quick overview for getting started with the Tavily tool. For a complete breakdown of the Tavily tool, you can find more detailed documentation in the API reference.



Overviewâ€‹

Integration detailsâ€‹

Class	Package	PY support	Package latest

TavilySearch	@langchain/tavily	âœ…	

Setupâ€‹



The integration lives in the @langchain/tavily package, which you can install as shown below:



TIP



See this section for general instructions on installing integration packages.



npm

yarn

pnpm

yarn add @langchain/tavily @langchain/core



Credentialsâ€‹



Set up an API key here and set it as an environment variable named TAVILY_API_KEY.



process.env.TAVILY_API_KEY = "YOUR_API_KEY";





Itâ€™s also helpful (but not needed) to set up LangSmith for best-in-class observability:



process.env.LANGSMITH_TRACING = "true";

process.env.LANGSMITH_API_KEY = "your-api-key";



Instantiationâ€‹



You can import and instantiate an instance of the TavilySearch tool like this:



import { TavilySearch } from "@langchain/tavily";



const tool = new TavilySearch({

  maxResults: 5,

  topic: "general",

  // includeAnswer: false,

  // includeRawContent: false,

  // includeImages: false,

  // includeImageDescriptions: false,

  // searchDepth: "basic",

  // timeRange: "day",

  // includeDomains: [],

  // excludeDomains: [],

});



Invocationâ€‹

Invoke directly with argsâ€‹



The Tavily search tool accepts the following arguments during invocation:



query (required): A natural language search query



The following arguments can also be set during invocation : includeImages, searchDepth , timeRange, includeDomains, excludeDomains, includeImages.



For reliability and performance reasons, certain parameters that affect response size cannot be modified during invocation: includeAnswer and includeRawContent. These limitations prevent unexpected context window issues and ensure consistent results.



await tool.invoke({

  query: "what is the current weather in SF?",

});



Invoke with ToolCallâ€‹



We can also invoke the tool with a model-generated ToolCall, in which case a ToolMessage will be returned:



// This is usually generated by a model, but we'll create a tool call directly for demo purposes.

const modelGeneratedToolCall = {

  args: {

    query: "what is the current weather in SF?",

  },

  id: "1",

  name: tool.name,

  type: "tool_call",

};



await tool.invoke(modelGeneratedToolCall);



Chainingâ€‹



We can use our tool in a chain by first binding it to a tool-calling model and then calling it:



Pick your chat model:

Groq

OpenAI

Anthropic

Google Gemini

FireworksAI

MistralAI

VertexAI

Install dependencies

TIP



See this section for general instructions on installing integration packages.



npm

yarn

pnpm

yarn add @langchain/groq 



Add environment variables

GROQ_API_KEY=your-api-key



Instantiate the model

import { ChatGroq } from "@langchain/groq";



const llm = new ChatGroq({

  model: "llama-3.3-70b-versatile",

  temperature: 0

});



import { HumanMessage } from "@langchain/core/messages";

import { ChatPromptTemplate } from "@langchain/core/prompts";

import { RunnableLambda } from "@langchain/core/runnables";



const prompt = ChatPromptTemplate.fromMessages([

  ["system", "You are a helpful assistant."],

  ["placeholder", "{messages}"],

]);



const llmWithTools = llm.bindTools([tool]);



const chain = prompt.pipe(llmWithTools);



const toolChain = RunnableLambda.from(async (userInput: string, config) => {

  const humanMessage = new HumanMessage(userInput);

  const aiMsg = await chain.invoke(

    {

      messages: [new HumanMessage(userInput)],

    },

    config

  );

  const toolMsgs = await tool.batch(aiMsg.tool_calls, config);

  return chain.invoke(

    {

      messages: [humanMessage, aiMsg, ...toolMsgs],

    },

    config

  );

});



const toolChainResult = await toolChain.invoke(

  "what is the current weather in sf?"

);



const { tool_calls, content } = toolChainResult;



console.log(

  "AIMessage",

  JSON.stringify(

    {

      tool_calls,

      content,

    },

    null,

    2

  )

);



Agentsâ€‹



For guides on how to use LangChain tools in agents, see the LangGraph.js docs.



API referenceâ€‹



For detailed documentation of all Tavily Search API features and configurations head to the API reference:



https://docs.tavily.com/documentation/api-reference/endpoint/search



Relatedâ€‹



Tool conceptual guide



Tool how-to guides



Was this page helpful?





You can also leave detailed feedback on GitHub.

Previous

Tavily Extract

Next

Tavily Search (Community, Deprecated)

Overview

Integration details

Setup

Credentials

Instantiation

Invocation

Invoke directly with args

Invoke with ToolCall

Chaining

Agents

API reference

Related

Community

Twitter

GitHub

Python

JS/TS

More

Homepage

Blog

Copyright Â© 2025 LangChain, Inc.



ComponentsTools/ToolkitsTavily Search

TavilySearch



Tavily is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed. Tavily offers two key endpoints, one of which being Search, which provides search results tailored for LLMs and RAG.



This guide provides a quick overview for getting started with the Tavily tool. For a complete breakdown of the Tavily tool, you can find more detailed documentation in the API reference.



Overviewâ€‹

Integration detailsâ€‹

Class	Package	PY support	Package latest

TavilySearch	@langchain/tavily	âœ…	

Setupâ€‹



The integration lives in the @langchain/tavily package, which you can install as shown below:



TIP



See this section for general instructions on installing integration packages.



npm

yarn

pnpm

yarn add @langchain/tavily @langchain/core



Credentialsâ€‹



Set up an API key here and set it as an environment variable named TAVILY_API_KEY.



process.env.TAVILY_API_KEY = "YOUR_API_KEY";





Itâ€™s also helpful (but not needed) to set up LangSmith for best-in-class observability:



process.env.LANGSMITH_TRACING = "true";

process.env.LANGSMITH_API_KEY = "your-api-key";



Instantiationâ€‹



You can import and instantiate an instance of the TavilySearch tool like this:



import { TavilySearch } from "@langchain/tavily";



const tool = new TavilySearch({

  maxResults: 5,

  topic: "general",

  // includeAnswer: false,

  // includeRawContent: false,

  // includeImages: false,

  // includeImageDescriptions: false,

  // searchDepth: "basic",

  // timeRange: "day",

  // includeDomains: [],

  // excludeDomains: [],

});



Invocationâ€‹

Invoke directly with argsâ€‹



The Tavily search tool accepts the following arguments during invocation:



query (required): A natural language search query



The following arguments can also be set during invocation : includeImages, searchDepth , timeRange, includeDomains, excludeDomains, includeImages.



For reliability and performance reasons, certain parameters that affect response size cannot be modified during invocation: includeAnswer and includeRawContent. These limitations prevent unexpected context window issues and ensure consistent results.



await tool.invoke({

  query: "what is the current weather in SF?",

});



Invoke with ToolCallâ€‹



We can also invoke the tool with a model-generated ToolCall, in which case a ToolMessage will be returned:



// This is usually generated by a model, but we'll create a tool call directly for demo purposes.

const modelGeneratedToolCall = {

  args: {

    query: "what is the current weather in SF?",

  },

  id: "1",

  name: tool.name,

  type: "tool_call",

};



await tool.invoke(modelGeneratedToolCall);



Chainingâ€‹



We can use our tool in a chain by first binding it to a tool-calling model and then calling it:



Pick your chat model:

Groq

OpenAI

Anthropic

Google Gemini

FireworksAI

MistralAI

VertexAI

Install dependencies

TIP



See this section for general instructions on installing integration packages.



npm

yarn

pnpm

yarn add @langchain/groq 



Add environment variables

GROQ_API_KEY=your-api-key



Instantiate the model

import { ChatGroq } from "@langchain/groq";



const llm = new ChatGroq({

  model: "llama-3.3-70b-versatile",

  temperature: 0

});



import { HumanMessage } from "@langchain/core/messages";

import { ChatPromptTemplate } from "@langchain/core/prompts";

import { RunnableLambda } from "@langchain/core/runnables";



const prompt = ChatPromptTemplate.fromMessages([

  ["system", "You are a helpful assistant."],

  ["placeholder", "{messages}"],

]);



const llmWithTools = llm.bindTools([tool]);



const chain = prompt.pipe(llmWithTools);



const toolChain = RunnableLambda.from(async (userInput: string, config) => {

  const humanMessage = new HumanMessage(userInput);

  const aiMsg = await chain.invoke(

    {

      messages: [new HumanMessage(userInput)],

    },

    config

  );

  const toolMsgs = await tool.batch(aiMsg.tool_calls, config);

  return chain.invoke(

    {

      messages: [humanMessage, aiMsg, ...toolMsgs],

    },

    config

  );

});



const toolChainResult = await toolChain.invoke(

  "what is the current weather in sf?"

);



const { tool_calls, content } = toolChainResult;



console.log(

  "AIMessage",

  JSON.stringify(

    {

      tool_calls,

      content,

    },

    null,

    2

  )

);



Agentsâ€‹



For guides on how to use LangChain tools in agents, see the LangGraph.js docs.



API referenceâ€‹



For detailed documentation of all Tavily Search API features and configurations head to the API reference:



https://docs.tavily.com/documentation/api-reference/endpoint/search



Relatedâ€‹



Tool conceptual guide



Tool how-to guides



Was this page helpful?





You can also leave detailed feedback on GitHub.

Previous

Tavily Extract

Next

Tavily Search (Community, Deprecated)

Overview

Integration details

Setup

Credentials

Instantiation

Invocation

Invoke directly with args

Invoke with ToolCall

Chaining

Agents

API reference

Related



ComponentsTools/ToolkitsTavily Search

TavilySearch



Tavily is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed. Tavily offers two key endpoints, one of which being Search, which provides search results tailored for LLMs and RAG.



This guide provides a quick overview for getting started with the Tavily tool. For a complete breakdown of the Tavily tool, you can find more detailed documentation in the API reference.



Overviewâ€‹

Integration detailsâ€‹

Class	Package	PY support	Package latest

TavilySearch	@langchain/tavily	âœ…	

Setupâ€‹



The integration lives in the @langchain/tavily package, which you can install as shown below:



TIP



See this section for general instructions on installing integration packages.



npm

yarn

pnpm

yarn add @langchain/tavily @langchain/core



Credentialsâ€‹



Set up an API key here and set it as an environment variable named TAVILY_API_KEY.



process.env.TAVILY_API_KEY = "YOUR_API_KEY";





Itâ€™s also helpful (but not needed) to set up LangSmith for best-in-class observability:



process.env.LANGSMITH_TRACING = "true";

process.env.LANGSMITH_API_KEY = "your-api-key";



Instantiationâ€‹



You can import and instantiate an instance of the TavilySearch tool like this:



import { TavilySearch } from "@langchain/tavily";



const tool = new TavilySearch({

  maxResults: 5,

  topic: "general",

  // includeAnswer: false,

  // includeRawContent: false,

  // includeImages: false,

  // includeImageDescriptions: false,

  // searchDepth: "basic",

  // timeRange: "day",

  // includeDomains: [],

  // excludeDomains: [],

});



Invocationâ€‹

Invoke directly with argsâ€‹



The Tavily search tool accepts the following arguments during invocation:



query (required): A natural language search query



The following arguments can also be set during invocation : includeImages, searchDepth , timeRange, includeDomains, excludeDomains, includeImages.



For reliability and performance reasons, certain parameters that affect response size cannot be modified during invocation: includeAnswer and includeRawContent. These limitations prevent unexpected context window issues and ensure consistent results.



await tool.invoke({

  query: "what is the current weather in SF?",

});



Invoke with ToolCallâ€‹



We can also invoke the tool with a model-generated ToolCall, in which case a ToolMessage will be returned:



// This is usually generated by a model, but we'll create a tool call directly for demo purposes.

const modelGeneratedToolCall = {

  args: {

    query: "what is the current weather in SF?",

  },

  id: "1",

  name: tool.name,

  type: "tool_call",

};



await tool.invoke(modelGeneratedToolCall);



Chainingâ€‹



We can use our tool in a chain by first binding it to a tool-calling model and then calling it:



Pick your chat model:

Groq

OpenAI

Anthropic

Google Gemini

FireworksAI

MistralAI

VertexAI

Install dependencies

TIP



See this section for general instructions on installing integration packages.



npm

yarn

pnpm

yarn add @langchain/groq 



Add environment variables

GROQ_API_KEY=your-api-key



Instantiate the model

import { ChatGroq } from "@langchain/groq";



const llm = new ChatGroq({

  model: "llama-3.3-70b-versatile",

  temperature: 0

});



import { HumanMessage } from "@langchain/core/messages";

import { ChatPromptTemplate } from "@langchain/core/prompts";

import { RunnableLambda } from "@langchain/core/runnables";



const prompt = ChatPromptTemplate.fromMessages([

  ["system", "You are a helpful assistant."],

  ["placeholder", "{messages}"],

]);



const llmWithTools = llm.bindTools([tool]);



const chain = prompt.pipe(llmWithTools);



const toolChain = RunnableLambda.from(async (userInput: string, config) => {

  const humanMessage = new HumanMessage(userInput);

  const aiMsg = await chain.invoke(

    {

      messages: [new HumanMessage(userInput)],

    },

    config

  );

  const toolMsgs = await tool.batch(aiMsg.tool_calls, config);

  return chain.invoke(

    {

      messages: [humanMessage, aiMsg, ...toolMsgs],

    },

    config

  );

});



const toolChainResult = await toolChain.invoke(

  "what is the current weather in sf?"

);



const { tool_calls, content } = toolChainResult;



console.log(

  "AIMessage",

  JSON.stringify(

    {

      tool_calls,

      content,

    },

    null,

    2

  )

);



Agentsâ€‹



For guides on how to use LangChain tools in agents, see the LangGraph.js docs.



API referenceâ€‹



For detailed documentation of all Tavily Search API features and configurations head to the API reference:



https://docs.tavily.com/documentation/api-reference/endpoint/search



Relatedâ€‹



Tool conceptual guide



Tool how-to guides